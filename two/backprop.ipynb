{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exponents = np.exp(x)\n",
    "    return exponents / np.sum(exponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W, b, activation = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, 1 dimensional vector) : Input Vector, outptut of the previous NN layer\n",
    "        W (np.array, 2 dimensional vector) : Weights of the layer\n",
    "        b (np.float32) : Bias value of the layer\n",
    "        activation (string) : The activation function of the layer, applied to the output\n",
    "    Returns:\n",
    "        W (np.array, 1 dimenstional vector) : The outputs of each neuron in the layer\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = X.shape[0]\n",
    "    output_shape = W.shape[0]\n",
    "\n",
    "    W = np.matmul(W.T, X) + b\n",
    "\n",
    "    if activation == 'ReLU':\n",
    "        W = ReLU(W)\n",
    "    elif activation == 'softmax':\n",
    "        W = softmax(W)\n",
    "\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23147525, 0.76852477], dtype=float32)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the forward pass function\n",
    "\n",
    "W1 = np.array([[1,2], [3,4]]).astype(np.float32)\n",
    "X1 = np.array([-1.8,3]).astype(np.float32)\n",
    "b1 = 0\n",
    "\n",
    "prediction = forward_pass(X=X1,\n",
    "             W=W1,\n",
    "             b=b1,\n",
    "             activation='softmax')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoricalCrossEntropy():\n",
    "    @staticmethod\n",
    "    def loss(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates and returns the loss of the prediction\n",
    "        Args:\n",
    "            y (int) : The label of the output (range: 0 to n-1)\n",
    "            y_pred (np.array, shape=(1,n)) : The output probabilities\n",
    "\n",
    "        Returns:\n",
    "            loss (np.float) : The categorical cross entropy loss wrt to given predictions\n",
    "        \"\"\"\n",
    "        return np.log(y_pred[y])\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y, y_pred):\n",
    "        \"\"\"\n",
    "        Returns the gradients of the loss wrt to the output probabilities\n",
    "        Args:\n",
    "            y (int) : The label of the output (range: 0 to n-1)\n",
    "            y_pred (np.array, shape=(n,)) : The output probabilities\n",
    "\n",
    "        Returns:\n",
    "            dL_dpred (array, shape=(n,)) : The gradients of loss wrt to the probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        gradient = np.zeros(y_pred.shape)\n",
    "        gradient[y] = -1 / y_pred[y]\n",
    "\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26328245"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the SparseCategoricalCrossEntropy Function\n",
    "Y_true = 1\n",
    "loss = SparseCategoricalCrossEntropy.loss(y=Y_true,\n",
    "                                 y_pred=prediction)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, W, b, output, activation, da):\n",
    "    if activation == None:\n",
    "        dz = da\n",
    "    elif activation == 'softmax':\n",
    "        dz = output * da\n",
    "    elif activation == 'ReLU':\n",
    "        dz = np.greater(output, 0) * da\n",
    "    else:\n",
    "        raise Exception(\"Activation funciton not found. Check the activation is either None, softmax or ReLU\")\n",
    "    \n",
    "    dw = np.matmul(X.reshape(-1,1), dz.reshape(1,-1))\n",
    "    return dz, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]),\n",
       " array([[ 0.        , -1.79999995],\n",
       "        [ 0.        ,  3.        ]]))"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the backward pass function\n",
    "da = [0, 1]\n",
    "backward_pass(X=X1,\n",
    "              W=W1,\n",
    "              b=b1,\n",
    "              output=prediction,\n",
    "              activation='ReLU',\n",
    "              da=da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_weights = pd.read_csv('Task_1/b/w-100-40-4.csv', header=None)\n",
    "# initial_weights = pd.read_csv('Task_1/a/w.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_biases =  pd.read_csv('Task_1/b/b-100-40-4.csv', header=None)\n",
    "# initial_biases = pd.read_csv('Task_1/a/b.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1]) # As given in data_point.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = initial_weights.iloc[0:14, 1:].to_numpy().astype(np.float32)\n",
    "w2 = initial_weights.iloc[14:114, 1:41].to_numpy().astype(np.float32)\n",
    "w3 = initial_weights.iloc[114:, 1:5].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = initial_biases.iloc[0, 1:].to_numpy().astype(np.float32)\n",
    "b2 = initial_biases.iloc[1, 1:41].to_numpy().astype(np.float32)\n",
    "b3 = initial_biases.iloc[2, 1:5].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label given for datapoint in data_point.txt\n",
    "y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLayer():\n",
    "    def __init__(self, input_shape, output_shape, activation=None, W=None, b=None, dtype=np.float64):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.X = np.zeros(input_shape)\n",
    "        self.W = np.random.randn(input_shape, output_shape) if W is None else W\n",
    "        self.b = np.random.randn(output_shape, 1) if b is None else b\n",
    "        self.activation = activation # One of None, 'ReLU' or 'softmax'\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "        self.dz = np.zeros(self.output_shape)\n",
    "        self.a = 0\n",
    "\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "    def display(self):\n",
    "         print(f\"This layer has an input shape of {self.input_shape}\")\n",
    "         print(f\"This layer has an output shape of {self.output_shape}\")\n",
    "         print(\"\\nThe Weights are, \")\n",
    "         print(self.W)\n",
    "         print(\"\\nThe biases are: \")\n",
    "         print(self.b)\n",
    "         print(\"\\nThe gradients of W are: \")\n",
    "         print(self.dW)\n",
    "         print(\"\\nThe gradients of b are: \")\n",
    "         print(self.db)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (np.array, 1 dimensional vector) : Input Vector, outptut of the previous NN layer\n",
    "\n",
    "        Returns:\n",
    "            W (np.array, 1 dimenstional vector) : The outputs of each neuron in the layer\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input, will be used during backpropogation\n",
    "        self.X = X\n",
    "\n",
    "        # Multiply the inputs by weights and add the bias\n",
    "        z = np.matmul(self.W.T, X) + self.b\n",
    "\n",
    "        # Apply the activation function\n",
    "        if self.activation == 'ReLU':\n",
    "            self.a = ReLU(z)\n",
    "        elif self.activation == 'softmax':\n",
    "            self.a = softmax(z)\n",
    "        else:\n",
    "            self.a = z\n",
    "            \n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gradient (np.array, 1 dimensional vector) : The gradient of loss wrt to the output of the layer dL/da \n",
    "        \n",
    "        Returns:\n",
    "            dL/dX (np.array, shape=(input_shape,)) : The gradient of loss wrt to the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        ## 1. First compute the gradient of loss wrt to z\n",
    "        if self.activation == None:\n",
    "            dz = gradient.astype(np.float32)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Compute the Jacobian, da/dz\n",
    "            J_da_dz = np.zeros((self.output_shape, self.output_shape))\n",
    "            for index, _ in np.ndenumerate(J_da_dz):\n",
    "                # J[i,j] = a_i * (I(i==j) - a_j)\n",
    "                i, j = index[0], index[1]\n",
    "                J_da_dz[index] = self.a[i] * ( (i==j) - self.a[j] )\n",
    "            dz = np.matmul(J_da_dz, gradient.reshape(-1,1)).astype(self.dtype)\n",
    "        elif self.activation == 'ReLU':\n",
    "            dz = (np.greater(self.a, 0) * gradient).astype(self.dtype)\n",
    "        else:\n",
    "            raise Exception(\"Activation funciton not found. Check the activation is either None, softmax or ReLU\")\n",
    "\n",
    "\n",
    "        ## 2. Use dL/dz to compute dw, db, dX\n",
    "        self.dW = np.matmul(self.X.reshape(-1,1), dz.reshape(1,-1)).astype(self.dtype)\n",
    "        self.db = dz.reshape(-1,1).astype(self.dtype)\n",
    "        dX = np.matmul(dz.reshape(1,-1), self.W.T).squeeze().astype(self.dtype)\n",
    "\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 4 # Inclusive of input and output layers\n",
    "LAYER_SIZES = [14, 100, 40, 4]\n",
    "\n",
    "layer1 = NNLayer(LAYER_SIZES[0], LAYER_SIZES[1], activation='ReLU', W=w1, b=b1, dtype=np.float32)\n",
    "layer2 = NNLayer(LAYER_SIZES[1], LAYER_SIZES[2], activation='ReLU', W=w2, b=b2, dtype=np.float32)\n",
    "layer3 = NNLayer(LAYER_SIZES[2], LAYER_SIZES[3], activation='softmax', W=w3, b=b3, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.88258338e-16, 1.08619921e-68, 1.47340802e-78, 1.00000000e+00])"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = layer1.forward(X)\n",
    "a2 = layer2.forward(a1)\n",
    "a3 = layer3.forward(a2)\n",
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient3 = SparseCategoricalCrossEntropy.gradient(y=y, y_pred=layer3.a)\n",
    "gradient2 = layer3.backward(gradient3)\n",
    "gradient1 = layer2.backward(gradient2)\n",
    "gradient0 = layer1.backward(gradient1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.8825834e-16],\n",
       "       [ 0.0000000e+00],\n",
       "       [ 0.0000000e+00],\n",
       "       [-1.1102230e-16]], dtype=float32)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Task_1/dw_new.csv', 'w') as f:\n",
    "    np.savetxt(f, layer1.dW, delimiter=',')\n",
    "    np.savetxt(f, layer2.dW, delimiter=',')\n",
    "    np.savetxt(f, layer3.dW, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Task_1/db_new.csv', 'w') as f:\n",
    "    np.savetxt(f, layer1.db.T, delimiter=',')\n",
    "    np.savetxt(f, layer2.db.T, delimiter=',')\n",
    "    np.savetxt(f, layer3.db.T, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154, 100), (3, 100))"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read these 2 csvs\n",
    "import pandas as pd\n",
    "dw = pd.read_csv('Task_1/dw_new.csv', header=None)\n",
    "db = pd.read_csv('Task_1/db_new.csv', header=None)\n",
    "\n",
    "dw.shape, db.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(self, X_train, y_train, X_test=None, y_test=None, epochs=100, learning_rate=0.1):\n",
    "        training_losses = []\n",
    "        training_accuracy = []\n",
    "        validation_losses = []\n",
    "        validation_accuracy = []\n",
    "        #### Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Training ... Epoch: {epoch} | \", end=\" \")\n",
    "\n",
    "            ### Forward pass\n",
    "            a = X_train.copy()\n",
    "            for layer in self.layers:\n",
    "                a = layer.forward(a)\n",
    "\n",
    "            ### Calculate the loss\n",
    "            loss, gradient = SparseCategoricalCrossEntropy.loss(y=y_train, y_pred=a, logits=True)\n",
    "            training_losses.append(loss)\n",
    "\n",
    "            accuracy = np.mean(np.argmax(self.predict(X_train, apply_softmax=True), axis=0) == y_train)\n",
    "            training_accuracy.append(accuracy)\n",
    "\n",
    "            print(\"Loss: \", loss, \" | Accuracy: \", accuracy)\n",
    "\n",
    "            ### Backward pass\n",
    "            for layer in reversed(self.layers):\n",
    "                gradient = layer.backward(gradient)\n",
    "                layer.update_parameters(training_samples=X_train.shape[1], learning_rate=learning_rate)\n",
    "\n",
    "\n",
    "            ### Validation\n",
    "            if X_test is not None and y_test is not None:\n",
    "                ### Forward pass\n",
    "                a_test = X_test\n",
    "                for layer in self.layers:\n",
    "                    a_test = layer.forward(a_test, inference=True)\n",
    "\n",
    "                loss_test, _ = SparseCategoricalCrossEntropy.loss(y=y_test, y_pred=a_test, logits=True)\n",
    "                validation_losses.append(loss_test)\n",
    "\n",
    "                accuracy= np.mean(np.argmax(self.predict(X_test, apply_softmax=True), axis=0) == y_test)\n",
    "                validation_accuracy.append(accuracy)\n",
    "\n",
    "\n",
    "        return training_losses, validation_losses, training_accuracy, validation_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
