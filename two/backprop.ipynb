{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ReLU(x):\n",
    "    return np.maximum(0, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exponents = np.exp(x)\n",
    "    return exponents / np.sum(exponents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(X, W, b, activation = None):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        X (np.array, 1 dimensional vector) : Input Vector, outptut of the previous NN layer\n",
    "        W (np.array, 2 dimensional vector) : Weights of the layer\n",
    "        b (np.float32) : Bias value of the layer\n",
    "        activation (string) : The activation function of the layer, applied to the output\n",
    "    Returns:\n",
    "        W (np.array, 1 dimenstional vector) : The outputs of each neuron in the layer\n",
    "    \"\"\"\n",
    "\n",
    "    input_shape = X.shape[0]\n",
    "    output_shape = W.shape[0]\n",
    "\n",
    "    W = np.matmul(W.T, X) + b\n",
    "\n",
    "    if activation == 'ReLU':\n",
    "        W = ReLU(W)\n",
    "    elif activation == 'softmax':\n",
    "        W = softmax(W)\n",
    "\n",
    "\n",
    "    return W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.23147525, 0.76852477], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the forward pass function\n",
    "\n",
    "W1 = np.array([[1,2], [3,4]]).astype(np.float32)\n",
    "X1 = np.array([-1.8,3]).astype(np.float32)\n",
    "b1 = 0\n",
    "\n",
    "prediction = forward_pass(X=X1,\n",
    "             W=W1,\n",
    "             b=b1,\n",
    "             activation='softmax')\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SparseCategoricalCrossEntropy():\n",
    "    @staticmethod\n",
    "    def loss(y, y_pred):\n",
    "        \"\"\"\n",
    "        Calculates and returns the loss of the prediction\n",
    "        Args:\n",
    "            y (int) : The label of the output (range: 0 to n-1)\n",
    "            y_pred (np.array, shape=(1,n)) : The output probabilities\n",
    "\n",
    "        Returns:\n",
    "            loss (np.float) : The categorical cross entropy loss wrt to given predictions\n",
    "        \"\"\"\n",
    "        return np.log(y_pred[y])\n",
    "    \n",
    "    @staticmethod\n",
    "    def gradient(y, y_pred):\n",
    "        \"\"\"\n",
    "        Returns the gradients of the loss wrt to the output probabilities\n",
    "        Args:\n",
    "            y (int) : The label of the output (range: 0 to n-1)\n",
    "            y_pred (np.array, shape=(n,)) : The output probabilities\n",
    "\n",
    "        Returns:\n",
    "            dL_dpred (array, shape=(n,)) : The gradients of loss wrt to the probabilities\n",
    "        \"\"\"\n",
    "\n",
    "        gradient = np.zeros(y_pred.shape)\n",
    "        gradient[y] = -1 / y_pred[y]\n",
    "\n",
    "        return gradient\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.26328245"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the SparseCategoricalCrossEntropy Function\n",
    "Y_true = 1\n",
    "loss = SparseCategoricalCrossEntropy.loss(y=Y_true,\n",
    "                                 y_pred=prediction)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(X, W, b, output, activation, da):\n",
    "    if activation == None:\n",
    "        dz = da\n",
    "    elif activation == 'softmax':\n",
    "        dz = output * da\n",
    "    elif activation == 'ReLU':\n",
    "        dz = np.greater(output, 0) * da\n",
    "    else:\n",
    "        raise Exception(\"Activation funciton not found. Check the activation is either None, softmax or ReLU\")\n",
    "    \n",
    "    dw = np.matmul(X.reshape(-1,1), dz.reshape(1,-1))\n",
    "    return dz, dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1]),\n",
       " array([[ 0.        , -1.79999995],\n",
       "        [ 0.        ,  3.        ]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the backward pass function\n",
    "da = [0, 1]\n",
    "backward_pass(X=X1,\n",
    "              W=W1,\n",
    "              b=b1,\n",
    "              output=prediction,\n",
    "              activation='ReLU',\n",
    "              da=da)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_weights = pd.read_csv('Task_1/b/w-100-40-4.csv', header=None)\n",
    "initial_weights = pd.read_csv('Task_1/a/w.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial_biases = initial_weights = pd.read_csv('Task_1/b/b-100-40-4.csv', header=None)\n",
    "initial_biases = pd.read_csv('Task_1/a/b.csv', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([-1, 1, 1, 1, -1, -1, 1, -1, 1, 1, -1, -1, 1, 1]) # As given in data_point.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "w1 = initial_weights.iloc[0:14, 1:].to_numpy().astype(np.float32)\n",
    "w2 = initial_weights.iloc[14:114, 1:41].to_numpy().astype(np.float32)\n",
    "w3 = initial_weights.iloc[114:, 1:5].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "b1 = initial_biases.iloc[0, 1:].to_numpy().astype(np.float32)\n",
    "b2 = initial_biases.iloc[1, 1:41].to_numpy().astype(np.float32)\n",
    "b3 = initial_biases.iloc[2, 1:5].to_numpy().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label given for datapoint in data_point.txt\n",
    "y = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNLayer():\n",
    "    def __init__(self, input_shape, output_shape, activation=None, W=None, b=None, dtype=np.float64):\n",
    "        self.input_shape = input_shape\n",
    "        self.output_shape = output_shape\n",
    "        self.X = np.zeros(input_shape)\n",
    "        self.W = np.random.randn(input_shape, output_shape) if W is None else W\n",
    "        self.b = np.random.randn(output_shape, 1) if b is None else b\n",
    "        self.activation = activation # One of None, 'ReLU' or 'softmax'\n",
    "        self.dW = np.zeros(self.W.shape)\n",
    "        self.db = np.zeros(self.b.shape)\n",
    "        self.dz = np.zeros(self.output_shape)\n",
    "        self.a = 0\n",
    "\n",
    "        self.dtype = dtype\n",
    "\n",
    "\n",
    "    def display(self):\n",
    "         print(f\"This layer has an input shape of {self.input_shape}\")\n",
    "         print(f\"This layer has an output shape of {self.output_shape}\")\n",
    "         print(\"\\nThe Weights are, \")\n",
    "         print(self.W)\n",
    "         print(\"\\nThe biases are: \")\n",
    "         print(self.b)\n",
    "         print(\"\\nThe gradients of W are: \")\n",
    "         print(self.dW)\n",
    "         print(\"\\nThe gradients of b are: \")\n",
    "         print(self.db)\n",
    "\n",
    "\n",
    "    def forward(self, X):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            X (np.array, 1 dimensional vector) : Input Vector, outptut of the previous NN layer\n",
    "\n",
    "        Returns:\n",
    "            W (np.array, 1 dimenstional vector) : The outputs of each neuron in the layer\n",
    "        \"\"\"\n",
    "\n",
    "        # Save the input, will be used during backpropogation\n",
    "        self.X = X\n",
    "\n",
    "        # Multiply the inputs by weights and add the bias\n",
    "        z = np.matmul(self.W.T, X) + self.b\n",
    "\n",
    "        # Apply the activation function\n",
    "        if self.activation == 'ReLU':\n",
    "            self.a = ReLU(z)\n",
    "        elif self.activation == 'softmax':\n",
    "            self.a = softmax(z)\n",
    "        else:\n",
    "            self.a = z\n",
    "            \n",
    "        return self.a\n",
    "    \n",
    "    def backward(self, gradient):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            gradient (np.array, 1 dimensional vector) : The gradient of loss wrt to the output of the layer dL/da \n",
    "        \n",
    "        Returns:\n",
    "            dL/dX (np.array, shape=(input_shape,)) : The gradient of loss wrt to the input of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        ## 1. First compute the gradient of loss wrt to z\n",
    "        if self.activation == None:\n",
    "            dz = gradient.astype(np.float32)\n",
    "        elif self.activation == 'softmax':\n",
    "            # Compute the Jacobian, da/dz\n",
    "            J_da_dz = np.zeros((self.output_shape, self.output_shape))\n",
    "            for index, _ in np.ndenumerate(J_da_dz):\n",
    "                # J[i,j] = a_i * (I(i==j) - a_j)\n",
    "                i, j = index[0], index[1]\n",
    "                J_da_dz[index] = self.a[i] * ( (i==j) - self.a[j] )\n",
    "            dz = np.matmul(J_da_dz, gradient.reshape(-1,1)).astype(self.dtype)\n",
    "        elif self.activation == 'ReLU':\n",
    "            dz = (np.greater(self.a, 0) * gradient).astype(self.dtype)\n",
    "        else:\n",
    "            raise Exception(\"Activation funciton not found. Check the activation is either None, softmax or ReLU\")\n",
    "\n",
    "\n",
    "        ## 2. Use dL/dz to compute dw, db, dX\n",
    "        self.dW = np.matmul(self.X.reshape(-1,1), dz.reshape(1,-1)).astype(self.dtype)\n",
    "        self.db = dz.reshape(-1,1).astype(self.dtype)\n",
    "        dX = np.matmul(dz.reshape(1,-1), self.W.T).squeeze().astype(self.dtype)\n",
    "\n",
    "\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_LAYERS = 4 # Inclusive of input and output layers\n",
    "LAYER_SIZES = [14, 100, 40, 4]\n",
    "\n",
    "layer1 = NNLayer(LAYER_SIZES[0], LAYER_SIZES[1], activation='ReLU', W=w1, b=b1, dtype=np.float32)\n",
    "layer2 = NNLayer(LAYER_SIZES[1], LAYER_SIZES[2], activation='ReLU', W=w2, b=b2, dtype=np.float32)\n",
    "layer3 = NNLayer(LAYER_SIZES[2], LAYER_SIZES[3], activation='softmax', W=w3, b=b3, dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.81297987e-158, 5.20887762e-137, 1.00000000e+000, 2.00080740e-085])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a1 = layer1.forward(X)\n",
    "a2 = layer2.forward(a1)\n",
    "a3 = layer3.forward(a2)\n",
    "a3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "gradient3 = SparseCategoricalCrossEntropy.gradient(y=y, y_pred=layer3.a)\n",
    "gradient2 = layer3.backward(gradient3)\n",
    "gradient1 = layer2.backward(gradient2)\n",
    "gradient0 = layer1.backward(gradient1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.],\n",
       "       [ 0.],\n",
       "       [ 1.],\n",
       "       [-1.]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer3.db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Task_1/dw.csv', 'w') as f:\n",
    "    np.savetxt(f, layer1.dW, delimiter=',')\n",
    "    np.savetxt(f, layer2.dW, delimiter=',')\n",
    "    np.savetxt(f, layer3.dW, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('Task_1/db.csv', 'w') as f:\n",
    "    np.savetxt(f, layer1.db.T, delimiter=',')\n",
    "    np.savetxt(f, layer2.db.T, delimiter=',')\n",
    "    np.savetxt(f, layer3.db.T, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((154, 100), (3, 100))"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read these 2 csvs\n",
    "import pandas as pd\n",
    "dw = pd.read_csv('Task_1/dw.csv', header=None)\n",
    "db = pd.read_csv('Task_1/db.csv', header=None)\n",
    "\n",
    "dw.shape, db.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
